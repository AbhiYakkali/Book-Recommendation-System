import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import correlation
from sklearn.metrics.pairwise import pairwise_distances
import ipywidgets as widgets
from IPython.display import display, clear_output
from contextlib import contextmanager
import warnings
warnings.filterwarnings('ignore')
import numpy as np
import os, sys
import re
from scipy.sparse import csr_matrix
 
#import data 
books = pd.read_csv('BX-CSV-Dump/BX-Books.csv', sep=';', error_bad_lines=False, encoding="latin-1")
books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL']
users = pd.read_csv('BX-CSV-Dump/BX-Users.csv', sep=';', error_bad_lines=False, encoding="latin-1")
users.columns = ['userID', 'Location', 'Age']
ratings = pd.read_csv('BX-CSV-Dump/BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding="latin-1")
ratings.columns = ['userID', 'ISBN', 'bookRating']
print(books.head()) 
print(books.shape)
print(list(books.columns))
 
#Data Preprocessing 
books.drop(['imageUrlS', 'imageUrlM', 'imageUrlL'], axis= 1, inplace= True)
books.columns= books.columns.str.strip().str.replace('-', '_')
users.columns= users.columns.str.strip().str.replace('-', '_')
ratings.columns= ratings.columns.str.strip().str.replace('-', '_')
print(books.head())  
 
# printing all year of publication to see whether there is error in yearofpublication
print(books.yearOfPublication.unique())
books.loc[books.yearOfPublication == 'DK Publishing Inc', :]#found error 
#  Data Cleaning, replace irrelevant values with most probable values
books.loc[books.ISBN == '0789466953','yearOfPublication '] = 2000
books.loc[books.ISBN == '0789466953','bookAuthor '] = "James Buckley"
books.loc[books.ISBN == '0789466953','publisher '] = "DK Publishing Inc"
books.loc[books.ISBN == '0789466953','bookTitle '] = "DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)James Buckley"
 
 

books.loc[books.ISBN == '078946697X','yearOfPublication '] = 2000
books.loc[books.ISBN == '078946697X','bookAuthor '] = "JMichael Teitelbaum"
books.loc[books.ISBN == '078946697X','publisher '] = "DK Publishing Inc"
books.loc[books.ISBN == '078946697X','bookTitle '] = "DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)\";Michael Teitelbaum"
 
#Found another error 
books.loc[books.yearOfPublication == 'Gallimard', :]
#cleaning
books.loc[books.ISBN == '2070426769','yearOfPublication '] = 2003
books.loc[books.ISBN == '2070426769','bookAuthor '] = "Jean-Marie Gustave"
books.loc[books.ISBN == '2070426769','publisher '] = "Gallimard"
books.loc[books.ISBN == '2070426769','bookTitle '] = "Peuple du ciel, suivi de Les Bergers"
 
#to convert yearofpublication into numeric
books.yearOfPublication = pd.to_numeric(books.yearOfPublication, errors = 'coerce')
 
#printing yearofpublication
print (sorted(books['yearOfPublication'].unique()))


# replacing the years which have more than 2006 because dataset belongs to 2004 year
books.loc[(books.yearOfPublication > 2006) | (books.yearOfPublication == 0), 'yearOfPublication'] = np.NAN
books.yearOfPublication.fillna(round(books.yearOfPublication.mean()), inplace = True)
books.yearOfPublication = books.yearOfPublication.astype(np.int32)
 
#checking the author data has null values or not
print(books.loc[(books['bookAuthor'].isnull()),: ])
 
 
#replacing with other for author which having NULL
books.loc[(books['ISBN'] == '9627982032'),'bookAuthor'] = 'other'
 
 
# Same with publisher
print(books.loc[books.publisher.isnull(),:])
books.loc[(books.ISBN == '193169656X'), 'publisher'] = 'other'
books.loc[(books.ISBN == '1931696993'), 'publisher'] = 'other'

#printing yearofpublication
print (sorted(books['yearOfPublication'].unique()))
 
# Printing Users Data
print(users.shape)
print(list(users.columns))
print(users.head())
 
 

#Age filtering who have more than 90 and less than 5 
print(sorted(users.Age.unique()))
users.loc[(users.Age > 90) | (users.Age < 5), 'Age'] = np.nan
users.Age = users.Age.fillna(users.Age.mean())
users.Age = users.Age.astype(np.int32)
print(sorted(users.Age.unique()))
 
 
#Ratings Dataset
print(ratings.shape)
print(ratings.shape)
print(list(ratings.columns))
print(ratings.head())
 
 
 
# Ratings Distribution
n_users = users.shape[0]
n_books = books.shape[0]
print(n_users * n_books)
new_ratings = ratings[ratings.ISBN.isin(books.ISBN)]
new_ratings = new_ratings[new_ratings.userID.isin(users.userID)]
print(new_ratings.head())
 
#Checking that the chance of book not missing after merging two datasets
sparsity = 1.0 - len(new_ratings)/float(n_users*n_books)
print( 'The sparsity level of Book Crossing Dataset is' + str(sparsity*100)+'%')
print(ratings.bookRating.unique())
 
 
 
# Implementing the Book Recommendation Using KNN Algorithm
 
#combine book data with rating data.
combine_book_rating = pd.merge(ratings, books, on = 'ISBN')
columns = ['bookAuthor','yearOfPublication', 'publisher']
combine_book_rating = combine_book_rating.drop(columns, axis = 1)
print(combine_book_rating.head())
 
 
#group by book titles and create a new column for total rating count.
combine_book_rating = combine_book_rating.dropna(axis = 0, subset = ['bookTitle'])
book_ratingcount = (combine_book_rating.groupby(by = ['bookTitle',])['bookRating'].count().reset_index().rename(columns = {'bookRating':'TotalRatingCount'})[['bookTitle','TotalRatingCount']])

print(book_ratingcount.head())
 
 
#Combine the rating data with the total rating count data, this gives us exactly 
# what  we need to filter out the less known books.
rating_with_totalratingcount = combine_book_rating.merge(book_ratingcount, left_on = 'bookTitle', right_on = 'bookTitle', how = 'inner' )
pd.set_option('display.float_format', lambda x: '%.3f' % x)
print(book_ratingcount[['TotalRatingCount']].describe())
 
 
#The median book has been rated only once. Let’s look at the top of the distribution:
print(book_ratingcount['TotalRatingCount'].quantile(np.arange(.9,1,.01)))
 

#About 1% of the books received 50 or more ratings. Because we have so many 
#books in our data, we will limit it to the top 1%, and this will give us 2713 unique 
#books.
popularity_threshold = 50
rating_popular_book = rating_with_totalratingcount.query('TotalRatingCount >= @popularity_threshold')
print(rating_popular_book.head())

print ((rating_popular_book['bookTitle'].unique()))
 
#Filtering to users in US and Canada only
# In order to improve computing speed, and not run into the “MemoryError”issue, 
#we are limiting our user data to those in the US and Canada. And then combine the 
#user data with rating data and total rating count data.
 
combined = rating_popular_book.merge(users, left_on = 'userID', right_on = 'userID', how = 'left')
us_canada_user_rating = combined[combined['Location'].str.contains("usa|canada")]
us_canada_user_rating = us_canada_user_rating.drop('Age', axis = 1)
print(us_canada_user_rating.tail(30))
if not us_canada_user_rating[us_canada_user_rating.duplicated(['userID', 'bookTitle'])].empty:
    initial_rows = us_canada_user_rating.shape[0]
    print('Initial dataframe shape {0}'.format(us_canada_user_rating.shape))
    us_canada_user_rating = us_canada_user_rating.drop_duplicates(['userID', 'bookTitle'])
    current_rows = us_canada_user_rating.shape[0]
    print('New dataframe shape {0}'.format(us_canada_user_rating.shape))
    print('Removed {0} rows'.format(initial_rows - current_rows))
us_canada_user_rating_pivot = us_canada_user_rating.pivot(index = 'bookTitle',columns = 'userID', values = 'bookRating').fillna(0)
us_canada_user_rating_matrix = csr_matrix(us_canada_user_rating_pivot.values)
 
 
#Finding the Nearest Neighbors for the books
from sklearn.neighbors import NearestNeighbors
model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')
model_knn.fit(us_canada_user_rating_matrix)
 
 
# Test our model and Make few Recommendations
print(us_canada_user_rating_pivot.head())
query_index = np.random.choice(us_canada_user_rating_pivot.shape[0])
distances, indices = model_knn.kneighbors(us_canada_user_rating_pivot.iloc[query_index, :].values.reshape(1, -1), n_neighbors = 6)
for i in range(0, len(distances.flatten())):
    if i == 0:
      print('Recommendations for {0}:\n'.format(us_canada_user_rating_pivot.index[query_index]))
    else:
       print('{0}: {1}, with distance of {2}:'.format(i, us_canada_user_rating_pivot.index[indices.flatten()[i]], distances.flatten()[i]))
us_canada_user_rating_pivot2 = us_canada_user_rating.pivot(index = 'userID', columns = 'bookTitle', values = 'bookRating').fillna(0)
print(us_canada_user_rating_pivot2.head())
print(us_canada_user_rating_pivot2.shape)
X = us_canada_user_rating_pivot2.values.T
print(X.shape)
 
 
#Now we are using SVD from Surprise module to decrease the dataset size to only important values
import sklearn
from sklearn.decomposition import TruncatedSVD
SVD = TruncatedSVD(n_components=12, random_state=17)
matrix = SVD.fit_transform(X)
print(matrix.shape)
 
import warnings
warnings.filterwarnings("ignore",category =RuntimeWarning)
corr = np.corrcoef(matrix)
print(corr.shape)
 
#Taking the input from user to give the similar book recommendation which have #good ratings
us_canada_book_title = us_canada_user_rating_pivot2.columns
us_canada_book_list = list(us_canada_book_title)
us_canada_book_list = [x.lower() for x in us_canada_book_list]
s=input("Please Enter the book to get the recommendation of similar books : ")
s=s.lower()
coffey_hands = us_canada_book_list.index(s)
corr_coffey_hands = corr[coffey_hands]
print(list(us_canada_book_title[(corr_coffey_hands<1.0) & (corr_coffey_hands>0.9)]))

  
#Graphs

 
#1
users.Age.hist(bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])
plt.title('Age Distribution\n')
plt.xlabel('age')
plt.ylabel('count')
plt.savefig('age_dist.png', bbox_inches='tight')
plt.show()
 
#2
new_ratings = ratings[ratings.ISBN.isin(books.ISBN)]
new_ratings = new_ratings[new_ratings.userID.isin(users.userID)]
ratings_explicit = new_ratings[new_ratings.bookRating != 0]
sns.countplot(data = ratings_explicit, x = 'bookRating')
plt.show()

 
 
#3
year = books.yearOfPublication.value_counts().sort_index()
year = year.where(year>5) 
plt.figure(figsize=(10, 8))
plt.rcParams.update({'font.size': 15}) 
plt.bar(year.index, year.values)
plt.xlabel('Year of Publication')
plt.ylabel('counts')
plt.show()

#4
plt.rc("font", size = 15)
ratings.bookRating.value_counts(sort = False).plot(kind = 'bar')
plt.title('Rating Distribution\n')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.savefig("Ratings Distribution.jpg", bbox_inches = "tight", dpi = 100)
plt.plot()

